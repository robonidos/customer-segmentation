{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic Approach for Clustering \n",
    "\n",
    "##### Step 1 - Handling Missing values\n",
    "\n",
    "calculating %age share of missing values against entire dataset\n",
    "dropping columns with missing % greater than threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import pprint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA    \n",
    "import operator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(12).reshape(4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [count, pct]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_data = df.isnull().sum()[df.isnull().sum() != 0]\n",
    "\n",
    "data_dictn = {'count': na_data.values, 'pct': np.round(na_data.values *100/(df.shape[0]),2)}\n",
    "\n",
    "df_null = pd.DataFrame(data=data_dictn, index=na_data.index)\n",
    "df_null.sort_values(by='count', ascending=False, inplace=True)\n",
    "df_null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outlier columns based on Null percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the outlier columns from the dataset. \n",
    "\n",
    "#Removing top 6 column based on the percentage of NaNs calculated in previous step\n",
    "drop_columns = ['TITEL_KZ', 'AGER_TYP', 'KK_KUNDENTYP', 'KBA05_BAUMAX', 'GEBURTSJAHR','ALTER_HH']\n",
    "df = df.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess Missing data in each row\n",
    "\n",
    "Divide the data into two subsets: one for data points that are above some threshold for missing values, and a second subset for points below that threshold.\n",
    "\n",
    "In order to know what to do with the outlier rows, we should see if the distribution of data values on columns that are not missing data (or are missing very little data) are similar or different between the two groups. Select at least five of these columns and compare the distribution of values\n",
    "\n",
    "If the distributions of non-missing features look similar between the data with many missing values and the data with few or no missing values, then we could argue that simply dropping those points from the analysis won't present a major issue. On the other hand, if the data with many missing values looks very different from the data with few or no missing values, then we should make a note on those data as special."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much data is missing in each row of the dataset?\n",
    "nan_rowcnt = df.isnull().sum(axis=1)\n",
    "nan_rowcnt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the missing columns, to determine the splitting of dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.hist(nan_rowcnt, bins=np.arange(0,50,1))\n",
    "plt.xlabel('NaNs')\n",
    "plt.ylabel('Row count')\n",
    "plt.xticks(np.arange(0,50,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the dataset at missing column count 25, since we have upto 50 missing columns \n",
    "msng_small = df[df.isnull().sum(axis=1) < 25].reset_index(drop=True)\n",
    "\n",
    "msng_large = df[df.isnull().sum(axis=1) >= 25].reset_index(drop=True)\n",
    "\n",
    "msng_large.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distribution of values for at least five columns where there are \n",
    "# no or few missing values, between the two subsets.\n",
    "\n",
    "col_names_small = msng_small.columns\n",
    "\n",
    "def print_countplot(cols,num):\n",
    "    \n",
    "    fig, axs = plt.subplots(num,2, figsize=(15, 15), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace =2 , wspace=.2)\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i in range(num):\n",
    "    \n",
    "        sns.countplot(msng_small[cols[i]], ax=axs[i*2])\n",
    "        axs[i*2].set_title('few_missing')\n",
    "        \n",
    "        sns.countplot(msng_large[cols[i]], ax=axs[i*2+1])\n",
    "        axs[i*2+1].set_title('high_missing')\n",
    "    \n",
    "    \n",
    "print_countplot(col_names_small,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to observe the distribution of data and decide whether to drop or include large missing value dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling missing values starting with visit year by randomly choosing a year accordin to its probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = df[df.VisitYear!=0].VisitYear.value_counts(normalize=1).sort_index()\n",
    "s2 = df[df.VisitYear == 0].index\n",
    "s3 = np.random.choice(a=s1.index.tolist(),p=s1.values.tolist(),size=s2.shape[0])\n",
    "#passing a dictionary in value field of replace\n",
    "df.VisitYear.replace(to_replace=0,value=dict(zip(s2,s3)),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Encode Features\n",
    "\n",
    "Since the unsupervised learning techniques to be used will only work on data that is encoded numerically, we need to make a few encoding changes to dataset.\n",
    "\n",
    "Typically, given 3 different type of variables (Numerical, Categorical and Mixed) - \n",
    "\n",
    "For numeric and interval data, these features can be kept without changes.\n",
    "For the ordinal variables - While ordinal values may technically be non-linear in spacing, make the simplifying assumption that the ordinal variables can be treated as being interval in nature (that is, kept without any changes).\n",
    "Special handling may be necessary for the remaining two variable types: categorical, and 'mixed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-encode categorical variables to be kept in the analysis.\n",
    "\n",
    "df = pd.get_dummies(df, columns=['CATEGORICAL_VARIABLE','CATEGORICAL_VARIABLE','CATEGORICAL_VARIABLE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Age Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_age(age):\n",
    "    if age < 18: return '0-17'\n",
    "    elif age < 25: return '18-24'\n",
    "    elif age < 35: return '25-34'\n",
    "    elif age < 45: return '35-44'\n",
    "    elif age < 55: return '45-54'\n",
    "    elif age < 65: return '55-64'\n",
    "    else: return '65+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_Category'] = df.AGE.apply(lambda x:categorize_age(x))  #AGE - is the age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the plot of members in different age categories\n",
    "df.groupby('Age_Category').MBRID.count().plot.bar( align='center',figsize=(18,9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICD9 Categorization\n",
    "\n",
    "Categorize 3500+ ICD9 codes among 20 categories refering to https://en.wikipedia.org/wiki/List_of_ICD-9_codes. Using get_dummies to get a binary table.\n",
    "\n",
    "Run quick analysis with the purpose to find patients that were diagnozed with one diagnosis multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_icd9code(code,method = 1):\n",
    "    icd9code = {    \n",
    "        '001-139': 'infectious and parasitic',\n",
    "        '140-239': 'neoplasms',\n",
    "        '240-279': 'endocrine, nutritional and metabolic, immunity disorders',\n",
    "        '280-289': 'diseases of the blood and blood-forming organs',\n",
    "        '290-319': 'mental disorders',\n",
    "        '320-359': 'nervous system',\n",
    "        '360-389': 'sense organs',\n",
    "        '390-459': 'circulatory system',\n",
    "        '460-519': 'respiratory system',\n",
    "        '520-579': 'digestive system',\n",
    "        '580-629': 'genitourinary system',\n",
    "        '630-679': 'complications of pregnancy, childbirth, and the puerperium',\n",
    "        '680-709': 'skin and subcutaneous tissue',\n",
    "        '710-739': 'musculoskeletal system and connective tissue',\n",
    "        '740-759': 'congenital anomalies',\n",
    "        '760-779': 'certain conditions originating in the perinatal period',\n",
    "        '780-799': 'symptoms, signs, and ill-defined conditions',\n",
    "        '800-999': 'injury and poisoning',\n",
    "        'E-V': 'external causes of injury and supplemental classification'\n",
    "    }\n",
    "    if method == 1:\n",
    "        code = code.split('.')[0]\n",
    "        if ('E' in code.upper()) or ('V' in code.upper()): return 'E-V'\n",
    "        elif int(code) < 139: return '001-139'\n",
    "        elif int(code) < 239: return '140-239'\n",
    "        elif int(code) < 279: return '240-279'\n",
    "        elif int(code) < 289: return '280-289'\n",
    "        elif int(code) < 319: return '290-319'\n",
    "        elif int(code) < 359: return '320-359'\n",
    "        elif int(code) < 389: return '360-389'\n",
    "        elif int(code) < 459: return '390-459'\n",
    "        elif int(code) < 519: return '460-519'\n",
    "        elif int(code) < 579: return '520-579'\n",
    "        elif int(code) < 629: return '580-629'\n",
    "        elif int(code) < 679: return '630-679'\n",
    "        elif int(code) < 709: return '680-709'\n",
    "        elif int(code) < 739: return '710-739'\n",
    "        elif int(code) < 759: return '740-759'\n",
    "        elif int(code) < 779: return '760-779'\n",
    "        elif int(code) < 799: return '780-799'\n",
    "        elif int(code) < 899: return '800-899'\n",
    "        else: return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ICD9CodeCategory'] = df.ICD9Code.apply(lambda x:categorize_icd9code(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_agg = df['ICD9CodeCategory']\n",
    "diagnosis_agg.index = df.MBRID\n",
    "diagnosis_agg = pd.get_dummies(diagnosis_agg,prefix='Icd9',prefix_sep='_').reset_index().groupby('MBRID').sum()\n",
    "\n",
    "s1 = diagnosis_agg.sum(axis=1) #sum of all the diagnosed categories per row\n",
    "\n",
    "s2 = (diagnosis_agg>0).sum(axis=1) #only where there's any diagnosis\n",
    "\n",
    "\n",
    "diagnosis_agg['DiagnosisCount'] = s1\n",
    "diagnosis_agg['VisitCount'] = s2\n",
    "diagnosis_agg['DiagnosisFreq'] = s1/s2\n",
    "\n",
    "#diagnosis_agg['AcuteCount'] = df1[['PatientGuid','Acute']].groupby('PatientGuid').sum()\n",
    "#diagnosis_agg['AcuteFreq'] = df1[['PatientGuid','Acute']].groupby('PatientGuid').sum()/df1[['PatientGuid','Acute']].groupby('PatientGuid').count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values with Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# removing NaNs using imputer\n",
    "imputer = Imputer(strategy='mean')\n",
    "imputed_df = imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we apply dimensionality reduction techniques to the data, we need to perform feature scaling so that the principal component vectors are not influenced by the natural differences in scale for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature scaling to the data.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized_df = scaler.fit_transform(imputed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimentionality reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First applying PCA to close to 50% of the features in data.\n",
    "pca = PCA(90)\n",
    "pca_ftr = pca.fit_transform(standardized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the variance accounted for by each principal component.\n",
    "def pca_variance_plot(pca):\n",
    "    # Creates a scree plot associated with the principal components \n",
    "    \n",
    "    #INPUT: pca - the result of instantian of PCA in scikit learn\n",
    "            \n",
    "    \n",
    "    num_cmpnt=len(pca.explained_variance_ratio_)\n",
    "    ind = np.arange(num_cmpnt)\n",
    "    vals = pca.explained_variance_ratio_\n",
    " \n",
    "    plt.figure(figsize=(18, 6))\n",
    "    ax = plt.subplot(111)\n",
    "    cumvals = np.cumsum(vals)\n",
    "    ax.bar(ind, vals)\n",
    "    ax.plot(ind, cumvals)\n",
    "    for i in range(num_cmpnt):\n",
    "        ax.annotate(r\"%s\" % ((str(vals[i]*100)[:3])), (ind[i], vals[i]), va=\"bottom\", ha=\"center\", fontsize=4.5)\n",
    " \n",
    "    ax.xaxis.set_tick_params(width=0)\n",
    "    ax.yaxis.set_tick_params(width=2, length=10)\n",
    " \n",
    "    ax.set_xlabel(\"Principal Component\")\n",
    "    ax.set_ylabel(\"Variance Explained (%)\")\n",
    "    plt.title('Explained Variance Per Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting PCA curve\n",
    "scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-apply PCA to the data while selecting for number of components to retain.\n",
    "pca = PCA(40)  # Select the number of features after which variance explained in the plot is stagnant\n",
    "pca_ftr = pca.fit_transform(standardized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature with absolute variance for a pca component\n",
    "def pca_plt(data, pca, n_compo):\n",
    "        \n",
    "    compo = pd.DataFrame(np.round(pca.components_, 5), columns = data.keys()).iloc[n_compo-1]\n",
    "    compo.sort_values(ascending=False, inplace=True)\n",
    "    compo = pd.concat([compo.head(6), compo.tail(6)])\n",
    "    \n",
    "    compo.plot(kind='bar', title='PCA for Component ' + str(n_compo))\n",
    "    ax = plt.gca()\n",
    "    ax.grid(linewidth='0.6', alpha=0.5)\n",
    "    ax.set_axisbelow(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_plt(df, pca, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Map weights for the second principal component to corresponding feature names\n",
    "# and then print the linked values, sorted by weight.\n",
    "\n",
    "pca_plt(df, pca, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map weights for the third principal component to corresponding feature names\n",
    "# and then print the linked values, sorted by weight.\n",
    "\n",
    "pca_plt(df, pca, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
